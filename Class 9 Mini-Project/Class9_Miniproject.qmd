---
title: "Class 9 Mini-Project"
author: "Adithi Kumar"
format: pdf
editor: visual
---

# Exploratory Data Analysis

```{r}
head(read.csv("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv"))  

fna.data <- "https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv" 

wisc.df <- read.csv (fna.data, row.names =1) 
 
head(wisc.df)
```

```{r}
#Get rid of first column (diagnosis) so it doesn't get in the way of our analysis 
wisc.data <- wisc.df[,-1]  
#Create a separate vector to store diagnosis column as a factor  (for later)
diagnosis <- as.factor(wisc.df [,1] )
head(diagnosis)
```

## Q1: How many observations are in this dataset?

```{r}
#The number of observations in wisc.data 
dim(wisc.data) 
 
```

There are 569 rows in the dataset so there are 569 (observations) patients

## Q2: How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)

```

There are 212 observations with a malignant diagnosis

## Q3: How many variables/features in the data are suffixed with `_mean`?

```{r}
pattern <- "_mean" 

#find the columns ending with _means
means <- grep(pattern, names(wisc.data), value =TRUE) 

 # count the number of columns 
length(means)

```

There are 10 features in the data that are suffixed with "\_mean".

# Principal Component Analysis

We need to scale our input data before PCA as somne of the columns are measured in terms of very different units with different means and difference variances. The upshot here is we set 'scale= TRUE' argument to 'prcomp()'.

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)

```

P

```{r}
#perform prcomp() onto wisc.data with scale!
wisc.pr <- prcomp(wisc.data, scale =T) 
z <- summary(wisc.pr)  
z

```

You can figure out how much a given observation affects the length and direction of PC1. The observations that are towards the end of the PC1 line are the ones that affect it the most ( basically the points that are farthest from the PC line midpoint.

## **Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

```{r}
z$importance[,1] 

```

The proportion of original variance captured by PC1 is about 44.272%.

## **Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
z


```

Three principal components are required to describe at least 70% of the original variance in the data; this is when the cumulative proportion is greater than 70%.

## **Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Seven principal components are required to describe at least 90% of the original variance in the data; this can be found by finding the first PC that has a cumulative proportion greater that 0.90.

## Q7: What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```

This plot looks very messy and impossible to read. It's hard to leave as all of the labels and patient IDs are overlapping.

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, pch =8, xlab ="PC1", ylab = "PC2")
```

## **Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis, pch =8, xlab ="PC1", ylab = "PC2")
```

These plots are a lot easier to read than the one created using 'biplot()'. From both of these plots, PC1 captures more of the variance of the data than PC2 or PC3. PC1 is better able to distinguish between the malignant and benign patients than PC2 or PC3.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
pr.vr <- wisc.pr$sdev^2 
head(pr.vr)
```

```{r}
 #Variance explained by each principal component: pve
pve <- pr.vr / sum(pr.vr)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

```

## **Q9.** For the first principal component, what is the component of the loading vector (i.e.Â `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?

```{r}
wisc.pr$rotation[8,1]
```

The concave.points.mean for PC1 in the rotation loading vector is 0.2608538.

## **Q10.** What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```

The minimum number of PCs required to explain 80% of the variance of the data is 5.

# Hierarchical Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)  

#Calculate Euclidean distance between the observations in the dataset
data.dist <- dist(data.scaled) 

#Use 'hclust()' with complete linkage
wisc.hclust <- hclust(data.dist, method ="complete")

```

## **Q11.** Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust) 
abline(h=19, col="red", lty=2) 

```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)  
#compare cluster membership to diagnosis 
table(wisc.hclust.clusters, diagnosis)
```

## **Q12.** Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, k=5)  
#compare cluster membership to diagnosis 
table(wisc.hclust.clusters2, diagnosis)
```

After trying multiple different cluster, the best cluster is k=4 because it is best able to separate the malignant and benign diagnoses into clusters without adding too many meaningless clusters.

## **Q13.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning.

```{r}
 w2 <- hclust(data.dist, method ="single") 
 plot(w2) 
 w3 <- hclust(data.dist, method ="average") 
plot(w3) 
 w4 <-  w2 <- hclust(data.dist, method ="ward.D2") 
 plot(w4)
```

I think the easiest visualization method is using "ward.D2". With such a large data set, it is hard to understand or even know where to start with most of the methods, but I think "ward.d2" does it the best. Also, by minimizing variance within clusters; it is able to minimize the risk of clustering errors. Something that the other methods don't do.

# Combining methods

```{r}
d <- dist(wisc.pr$x[,1:7]) 
wisc.pr.hclust <- hclust(d, method = "ward.D2") 
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object.

```{r}
grps <- cutree(wisc.pr.hclust, k =2) 

plot(wisc.pr$x[,1], wisc.pr$x[,2], col =grps)
```

```{r}
table(grps)
```

## Q15: How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```

There is a clear distinction between which cluster the malignant observations tend to be and which cluster the benign observations tend to be. For the most part, cluster 1 is assigned with benign diagnoses while cluster 2 is assigned to the malignant diagnoses. The table can be used to figure out the likelihood of false positives and false negatives by looking at the number of observations that are benign in cluster 1 and malignant in cluster 2.
